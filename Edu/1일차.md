### Tensor

미분 기하학 대상

파이토치 행렬이라 생각하면 됨

```python
from __future__ import print_function
import torch

x = torch.empty(5,3)
print(x)
```

```
tensor([[7.4699e-36, 0.0000e+00, 3.3631e-44],
        [0.0000e+00,        nan, 0.0000e+00],
        [1.1578e+27, 1.1362e+30, 7.1547e+22],
        [4.5828e+30, 1.2121e+04, 7.1846e+22],
        [9.2198e-39, 0.0000e+00, 0.0000e+00]])
```

초기화 되지 않은 행렬 생성

`torch.rand`초기화된 배열 생성

### torch, numpy 변환

```python
a = torch.ones(5)
print(a)
b = a.numpy()
print(b)
## a = torch.from_numpy(b)
```

### cuda 사용

```python
import torch
x = torch.randn(1)
if torch.cuda.is_available():
  device = torch.device("cuda")
  y = torch.ones_like(x, device=device)
  x = x.to(device)
  z = x + y
  print(z)
  print(z.to("cpu",torch.double))
```

### autograd

Tensor의 모든 연산에 대해 자동 미분 제공

실행-기반-정의 프레임워크

##### Tensor

패키지 중심에는 `torch.Tensor`클래스가 있다. 

`requires_grad`속성을 True로 하면 그 tensor에서 이루어진 모든 연산을 추적한다

계산 완료 후 `.backward()`를 호출하여 모든 변화도를 자동으로 계산할 수 있다.

이 변화도는 `.grad()`에 누적

기록 중단  = `.detach`

`Function`클래스 = `Tensor`와 상호 연결됨, 모든 연산과정 부호화 하여 순환하지 않은 그래프 생성

도함수 계산하기 위해서는 `.backard()`호출하면 된다

`Tensor`가 스칼라인 경우에는 `backward`에 인자 정해줄 필요 없다.

